import streamlit as st
import base64
import google.auth
from google.oauth2 import service_account
import google.auth.transport.requests
import vertexai
from vertexai.generative_models import GenerativeModel, Part, FinishReason
import vertexai.preview.generative_models as generative_models
import json
import logging

def process_credentials(cred_dict):
    # å¤„ç†å¤šè¡Œç§é’¥
    if 'private_key' in cred_dict:
        cred_dict['private_key'] = cred_dict['private_key'].replace('\\n', '\n')
    return cred_dict

try:
    # è·å– TOML æ ¼å¼çš„å‡­è¯ä¿¡æ¯
    credentials_toml = st.secrets["GOOGLE_APPLICATION_CREDENTIALS"]
    
    # å°† TOML æ ¼å¼è½¬æ¢ä¸ºå­—å…¸å¹¶å¤„ç†
    credentials_dict = process_credentials(dict(credentials_toml))
    
    # åˆ›å»ºå‡­è¯å¯¹è±¡
    creds = service_account.Credentials.from_service_account_info(
        credentials_dict,
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )
    
    # éªŒè¯å‡­è¯
    auth_req = google.auth.transport.requests.Request()
    creds.refresh(auth_req)
    
    st.success("Successfully loaded and verified credentials!")
    
    # æ˜¾ç¤ºéæ•æ„Ÿçš„å‡­è¯ä¿¡æ¯
    safe_info = {k: v for k, v in credentials_dict.items() if k not in ['private_key', 'private_key_id']}
    st.write("Partial credential info (non-sensitive):", json.dumps(safe_info, indent=2))

except Exception as e:
    st.error(f"An error occurred while loading the credentials: {str(e)}")
    st.error("Please check the application logs for more details.")
    
    # æ˜¾ç¤ºéæ•æ„Ÿçš„å‡­è¯ä¿¡æ¯ï¼ˆå³ä½¿åœ¨é”™è¯¯æƒ…å†µä¸‹ï¼‰
    if 'credentials_dict' in locals():
        safe_info = {k: v for k, v in credentials_dict.items() if k not in ['private_key', 'private_key_id']}
        st.write("Partial credential info (non-sensitive):", json.dumps(safe_info, indent=2))

# Streamlit åº”ç”¨ç•Œé¢
left_co, cent_co,last_co = st.columns([0.39,0.31,0.30])
with cent_co:
    st.title(":blue[GBB] :rainbow[AI]")
left_co, cent_co,last_co = st.columns([0.39,0.31,0.3])
with cent_co:
    st.caption(":blue[_ä¼ä¸šçº§å†…å®¹ç”Ÿæˆå¹³å°_]")
st.image('https://storage.googleapis.com/ghackathon/page_0.png')
left_co, cent_co,last_co = st.columns([0.24,0.51,0.25])
with cent_co:
    st.subheader('', divider='rainbow')
    
#ç»§ç»­streamlit sidebarç•Œé¢
with st.sidebar:
    left_co, cent_co,last_co = st.columns([0.34,0.33,0.33])
    with cent_co:
        st.image('https://storage.googleapis.com/ghackathon/image2.gif')
    left_co, cent_co,last_co = st.columns([0.36,0.32,0.32])
    with cent_co:
        st.title(":blue[GBB] :rainbow[AI]")
    temperature = st.slider("è°ƒæ•´æ¨¡å‹Temperature", min_value=0.0, max_value=2.0, value=1.5, help=(
        """
        Temperatureç”¨äºå“åº”ç”ŸæˆæœŸé—´çš„é‡‡æ ·ï¼Œè¿™å‘ç”Ÿåœ¨åº”ç”¨ topP å’Œ topK æ—¶ã€‚Temperatureæ§åˆ¶äº†tokené€‰æ‹©ä¸­çš„éšæœºç¨‹åº¦ã€‚å¯¹äºéœ€è¦è¾ƒå°‘å¼€æ”¾å¼æˆ–åˆ›é€ æ€§å“åº”çš„æç¤ºï¼Œè¾ƒä½çš„temperatureæ˜¯å¥½çš„ï¼Œè€Œè¾ƒé«˜çš„temperatureå¯ä»¥å¯¼è‡´æ›´å¤šæ ·åŒ–æˆ–åˆ›é€ æ€§çš„ç»“æœã€‚Temperatureä¸º 0 æ„å‘³ç€å§‹ç»ˆé€‰æ‹©æœ€é«˜æ¦‚ç‡çš„tokenã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç»™å®šæç¤ºçš„å“åº”å¤§å¤šæ˜¯ç¡®å®šçš„ï¼Œä½†ä»æœ‰å¯èƒ½å‡ºç°å°‘é‡å˜åŒ–ã€‚
        
        å¦‚æœæ¨¡å‹è¿”å›çš„å“åº”è¿‡äºé€šç”¨ã€å¤ªçŸ­æˆ–æ¨¡å‹ç»™å‡ºå›é€€å“åº”ï¼Œè¯·å°è¯•æé«˜temperatureã€‚
        """
    ))
    top_p = st.slider ("è°ƒæ•´æ¨¡å‹Top_p", min_value=0.00, max_value=1.00, value=0.95, help=(
        """
        Top-P æ”¹å˜äº†æ¨¡å‹é€‰æ‹©è¾“å‡ºtokensçš„æ–¹å¼ã€‚TokensæŒ‰ç…§ä»æœ€å¯èƒ½ï¼ˆè§top-Kï¼‰åˆ°æœ€ä¸å¯èƒ½çš„é¡ºåºè¿›è¡Œé€‰æ‹©ï¼Œç›´åˆ°å®ƒä»¬çš„æ¦‚ç‡ä¹‹å’Œç­‰äºtop-På€¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœtoken Aã€Bå’ŒCçš„æ¦‚ç‡åˆ†åˆ«ä¸º0.3ã€0.2å’Œ0.1ï¼Œtop-På€¼ä¸º0.5ï¼Œé‚£ä¹ˆæ¨¡å‹å°†ä½¿ç”¨æ¸©åº¦ä»Aæˆ–Bä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªtokenï¼Œå¹¶æ’é™¤Cä½œä¸ºå€™é€‰ã€‚

        æŒ‡å®šè¾ƒä½çš„å€¼ä¼šå¾—åˆ°è¾ƒå°‘çš„éšæœºå“åº”ï¼ŒæŒ‡å®šè¾ƒé«˜çš„å€¼ä¼šå¾—åˆ°æ›´å¤šçš„éšæœºå“åº”ã€‚
        """
    ))
    st.subheader('',divider='rainbow')
    st.text("")
    st.page_link("homepage.py", label="ä¸»é¡µ", icon="ğŸ ")
    st.page_link("pages/page_1.py", label="æ–‡æœ¬ç”Ÿæˆ", icon="ğŸ“–")
    st.page_link("pages/page_2.py", label="è§†é¢‘ç†è§£", icon="ğŸï¸")
    st.page_link("pages/page_3.py", label="æ–‡æœ¬ç¿»è¯‘", icon="ğŸ‡ºğŸ‡³")
    st.page_link("pages/page_4.py", label="RAGæœç´¢", icon="ğŸ”")
    st.page_link("pages/page_5.py", label="åª’ä½“æœç´¢", icon="ğŸ¥")
    st.page_link("pages/page_6.py", label="å›¾ç‰‡ç”Ÿæˆ", icon="ğŸ¨")
    st.page_link("pages/page_7.py", label="èŠå¤©æœºå™¨äºº", icon="ğŸ’¬")
    st.page_link("pages/page_8.py", label="æ¸¸æˆå®¢æœå¹³å°", icon="ğŸ¤–")
    st.page_link("pages/page_9.py", label="ç”µå•†å®¢æœå¹³å°", icon="ğŸ¤–")
    st.page_link("pages/page_10.py", label="Claude3.5èŠå¤©æœºå™¨äºº", icon="ğŸ’¬")
    st.page_link("pages/page_11.py", label="Llama3.1èŠå¤©æœºå™¨äºº", icon="ğŸ’¬")
    st.page_link("https://pantheon.corp.google.com/translation/hub", label="GCPç¿»è¯‘é—¨æˆ·", icon="ğŸŒ")
    st.page_link("https://pantheon.corp.google.com/vertex-ai/generative/multimodal/gallery", label="GCPæ§åˆ¶å° - Gemini", icon="ğŸŒ")
    st.page_link("https://pantheon.corp.google.com/gen-app-builder/engines", label="GCPæ§åˆ¶å° - App Builder", icon="ğŸŒ")
    st.text("")
    st.subheader('', divider='rainbow')
    st.text("")
    left_co, cent_co,last_co = st.columns([0.39,0.31,0.30])
    with cent_co:
        st.write('Â© GBB')
    left_co, cent_co,last_co = st.columns([0.09,0.83,0.08])
    with cent_co:
        st.write(':grey[Designed & Developed by] :blue[ææ–‡åº·]')
    left_co, cent_co,last_co = st.columns([0.22,0.6,0.18])
    with cent_co:
        st.write(':grey[Powered by] **Vertex AI**')

# å®šä¹‰ç”Ÿæˆæ–‡æœ¬çš„å‡½æ•°
def generate_text(prompt):
  vertexai.init(project="lwk-genai-test", location="us-central1")
  model = GenerativeModel("gemini-1.5-flash-001")
  responses = model.generate_content(
      [prompt],
      generation_config=generation_config,
      safety_settings=safety_settings,
      stream=True,
  )

  generated_text = ""
  for response in responses:
    generated_text += response.text

  return generated_text

# å®šä¹‰æ¨¡å‹å‚æ•°
generation_config = {
    "max_output_tokens": 8192,
    "temperature": temperature,
    "top_p": top_p,
}

safety_settings = {
    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
}

#ç»§ç»­streamlitç•Œé¢
prompt = st.text_area("è¯·è¾“å…¥æ‚¨çš„æç¤ºè¯ï¼š", "")

uploaded_files = st.file_uploader("å¦‚æœæ‚¨éœ€è¦å¤„ç†æ–‡æ¡£ï¼Œè¯·åœ¨è¿™é‡Œä¸Šä¼ ï¼Œå¯ä»¥åŒæ—¶é€‰æ‹©å¤šä»½æ–‡æ¡£ä¸Šä¼ ï¼š", type=("txt"), accept_multiple_files=True)

if uploaded_files:
    all_text = ""
    for uploaded_file in uploaded_files:
        bytes_data = uploaded_file.read()
        text = bytes_data.decode()
        all_text += text + "\n\n"
            

with st.form("myform"):
    left_co, cent_co,last_co = st.columns([0.42,0.29,0.29])
    with cent_co:
        submitted = st.form_submit_button("ç”Ÿæˆæ–‡æœ¬")
    if uploaded_files and submitted and not prompt:
        st.info("è¯·è¾“å…¥æç¤ºè¯")
    
    if prompt and submitted and not uploaded_files:
        prompt_without_article = f'å¦‚æœæˆ‘é—®ä½ ä½ æ˜¯è°ï¼Œè¯·ç›´æ¥å›ç­”æˆ‘â€œæˆ‘æ˜¯GBB AIï¼Œç”±Google Geminié©±åŠ¨çš„å¤§è¯­è¨€æ¨¡å‹ã€‚â€ï¼Œå¦‚æœæˆ‘é—®ä½ å…³äº"GBB"çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”"GBBä»£è¡¨Go Beyond Blueï¼Œå°±æ˜¯â€œæ·±è“â€çš„æ„æ€ã€‚",å¦‚æœæˆ‘æ²¡æœ‰é—®ä½ ä½ æ˜¯è°æˆ–è€…å…³äº"GBB"çš„é—®é¢˜ï¼Œé‚£ä¹ˆä¸éœ€è¦å›ç­”å‰é¢çš„å†…å®¹ï¼Œä¹Ÿä¸éœ€è¦å¼•ç”¨å‰é¢çš„å†…å®¹ï¼Œè¯·ç›´æ¥æ ¹æ®æ¥ä¸‹æ¥çš„"æç¤ºè¯"å›ç­”æˆ‘ï¼š\n\n æç¤ºè¯: \n{prompt}\n\nå›ç­”ï¼š'
        with st.spinner('è¯·ç¨ç­‰ :coffee: å†…å®¹é©¬ä¸Šå°±æ¥...'):
            generated_text = generate_text(prompt_without_article)
            st.write(generated_text)
            
    if prompt and submitted and uploaded_files:
        prompt_with_article = f'å¦‚æœæˆ‘é—®ä½ ä½ æ˜¯è°ï¼Œè¯·ç›´æ¥å›ç­”æˆ‘â€œæˆ‘æ˜¯GBB AIï¼Œç”±Google Geminié©±åŠ¨çš„å¤§è¯­è¨€æ¨¡å‹ã€‚â€ï¼Œå¦‚æœæˆ‘é—®ä½ å…³äº"GBB"çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”"GBBä»£è¡¨Go Beyond Blueï¼Œå°±æ˜¯â€œæ·±è“â€çš„æ„æ€ã€‚",å¦‚æœæˆ‘æ²¡æœ‰é—®ä½ ä½ æ˜¯è°æˆ–è€…å…³äº"GBB"çš„é—®é¢˜ï¼Œé‚£ä¹ˆä¸éœ€è¦å›ç­”å‰é¢çš„å†…å®¹ï¼Œä¹Ÿä¸éœ€è¦å¼•ç”¨å‰é¢çš„å†…å®¹ï¼Œè¯·ç›´æ¥æ ¹æ®æ¥ä¸‹æ¥çš„"æç¤ºè¯"å›ç­”æˆ‘ï¼š\n{all_text}\n\n æç¤ºè¯: \n{prompt}\n\nå›ç­”ï¼š'
        with st.spinner('è¯·ç¨ç­‰ :coffee: å†…å®¹é©¬ä¸Šå°±æ¥...'):
            generated_text = generate_text(prompt_with_article)
            st.write(generated_text)
