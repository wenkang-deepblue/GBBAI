import streamlit as st
import base64
import google.auth
from google.oauth2 import service_account
import google.auth.transport.requests
import vertexai
from vertexai.generative_models import GenerativeModel, Part, FinishReason
import vertexai.preview.generative_models as generative_models
import json
import logging
from auth import login, callback, logout

if "code" in st.query_params:
    callback()

if not login():
    st.stop()

with st.sidebar:
    st.markdown(f"""
        <div style="background-color: #d4edda; border-color: #c3e6cb; color: #155724; 
                    padding: 10px; border-radius: 0.25rem; text-align: center; margin-bottom: 10px;">
            <p style="margin-bottom: 0;">Welcome!</p>
        </div>
    """, unsafe_allow_html=True)
    left_co, cent_co,last_co = st.columns([0.35,0.33,0.32])
    with cent_co:
        if st.button("log out"):
            logout()

credentials_info = st.secrets["GOOGLE_APPLICATION_CREDENTIALS"]

creds = service_account.Credentials.from_service_account_info(
    credentials_info,
    scopes=["https://www.googleapis.com/auth/cloud-platform"]
)

auth_req = google.auth.transport.requests.Request()
creds.refresh(auth_req)

vertexai.init(project="lwk-genai-test", location="us-central1", credentials=creds)

# Streamlit application interface
left_co, cent_co,last_co = st.columns([0.35,0.35,0.3])
with cent_co:
    st.title(":blue[GCP Gen]:rainbow[AI]")
left_co, cent_co,last_co = st.columns([0.2,0.6,0.2])
with cent_co:
    st.caption(":blue[_Enterprise-ready Content Generation Platform_]")
st.image('https://storage.googleapis.com/ghackathon/page_0.png')
left_co, cent_co,last_co = st.columns([0.24,0.51,0.25])
with cent_co:
    st.subheader('', divider='rainbow')
    
# Continue streamlit sidebar interface
with st.sidebar:
    left_co, cent_co,last_co = st.columns([0.34,0.33,0.33])
    with cent_co:
        st.image('https://storage.googleapis.com/ghackathon/image2.gif')
    left_co, cent_co,last_co = st.columns([0.28,0.5,0.22])
    with cent_co:
        st.title(":blue[GCP Gen]:rainbow[AI]")
    temperature = st.slider("Adjust Model Temperature", min_value=0.0, max_value=2.0, value=1.5, help=(
        """
        Temperature is used for sampling during response generation, which occurs when applying topP and topK. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require less open-ended or creative responses, while higher temperatures can lead to more diverse or creative results. A temperature of 0 means always selecting the highest probability token. In this case, the response for a given prompt is mostly deterministic, but there may still be some minor variations.
        
        If the model returns responses that are too generic, too short, or the model gives fallback responses, try increasing the temperature.
        """
    ))
    top_p = st.slider ("Adjust Model Top_p", min_value=0.00, max_value=1.00, value=0.95, help=(
        """
        Top-P changes how the model selects output tokens. Tokens are chosen in order from most likely (see top-K) to least likely until their cumulative probability equals the top-P value. For example, if tokens A, B, and C have probabilities of 0.3, 0.2, and 0.1 respectively, and the top-P value is 0.5, the model will use temperature to choose the next token from either A or B, excluding C as a candidate.

        Specifying a lower value will result in less random responses, while specifying a higher value will result in more random responses.
        """
    ))
    st.subheader('',divider='rainbow')
    st.text("")
    st.page_link("homepage.py", label="Home", icon="üè†")
    st.page_link("pages/page_01_text_generation.py", label="Text Generation", icon="üìñ")
    st.page_link("pages/page_02_media_understanding.py", label="Media Understanding", icon="üéûÔ∏è")
    st.page_link("pages/page_03_translation.py", label="Text Translation", icon="üá∫üá≥")
    st.page_link("pages/page_04_travel_advisor.py", label="Travel Advisor", icon="‚úàÔ∏è")
    st.page_link("pages/page_05_rag_search.py", label="RAG Search", icon="üîç")
    st.page_link("pages/page_06_media_search.py", label="Media Search", icon="üé•")
    st.page_link("pages/page_07_image_generation.py", label="Image Generation", icon="üé®")
    st.page_link("pages/page_08_chatbot.py", label="Chatbot", icon="üí¨")
    st.page_link("pages/page_09_gaming_servicebot.py", label="Gaming Servicebot", icon="ü§ñ")
    st.page_link("pages/page_10_ecommerce_servicebot.py", label="E-commerce Servicebot", icon="ü§ñ")
    st.page_link("pages/page_11_claude_chatbot.py", label="Claude 3.5 Chatbot", icon="üí¨")
    st.page_link("pages/page_12_llama_chatbot.py", label="Llama 3.1 Chatbot", icon="üí¨")
    st.page_link("https://pantheon.corp.google.com/translation/hub", label="GCP Translation Hub", icon="üåé")
    st.page_link("https://pantheon.corp.google.com/vertex-ai/generative/multimodal/gallery", label="GCP Console - Gemini", icon="üåé")
    st.page_link("https://pantheon.corp.google.com/gen-app-builder/engines", label="GCP Console - App Builder", icon="üåé")
    st.text("")
    st.subheader('', divider='rainbow')
    st.text("")
    st.text("")
    st.markdown(
        """
    ## About
    This is a generative AI platform powered by Google Cloud Vertex AI and an enterprise-ready RAG search engine
    """
    )
    st.page_link("https://cloud.google.com/vertex-ai?hl=en", label="Google Cloud Vertex AI", icon="‚òÅÔ∏è")
    left_co, cent_co,last_co = st.columns([0.41,0.31,0.28])
    with cent_co:
        st.write('¬© LWK')
    left_co, cent_co,last_co = st.columns([0.09,0.83,0.08])
    with cent_co:
        st.markdown(
        f'<p style="text-align: center;">'
        f'<span style="color: grey;">Designed & Developed by</span> '
        f'<a href="{st.secrets["developer_profile_link"]}" '
        f'style="color: #185ABC; text-decoration: underline;" target="_blank">{st.secrets["developer_name"]}</a>'
        f'</p>',
        unsafe_allow_html=True
    )
    left_co, cent_co,last_co = st.columns([0.22,0.6,0.18])
    with cent_co:
        st.write(':grey[Powered by] **Vertex AI**')

    st.page_link("pages/terms_of_service.py", label="Terms of Service", icon="üìÑ")
    st.page_link("pages/privacy_policy.py", label="Privacy Policy", icon="üîí")

# Define function to generate text
def generate_text(prompt):
  vertexai.init(project="lwk-genai-test", location="us-central1")
  model = GenerativeModel("gemini-1.5-flash-001")
  responses = model.generate_content(
      [prompt],
      generation_config=generation_config,
      safety_settings=safety_settings,
      stream=True,
  )

  generated_text = ""
  for response in responses:
    generated_text += response.text

  return generated_text

# Define model parameters
generation_config = {
    "max_output_tokens": 8192,
    "temperature": temperature,
    "top_p": top_p,
}

safety_settings = {
    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
}

# Continue streamlit interface
prompt = st.text_area("Please input your prompt:", "")

uploaded_files = st.file_uploader("If you need to process documents, please upload here. You can select multiple documents to upload:", type=("txt"), accept_multiple_files=True)

if uploaded_files:
    all_text = ""
    for uploaded_file in uploaded_files:
        bytes_data = uploaded_file.read()
        text = bytes_data.decode()
        all_text += text + "\n\n"
            

with st.form("myform"):
    left_co, cent_co,last_co = st.columns([0.42,0.29,0.29])
    with cent_co:
        submitted = st.form_submit_button("Generate Text")
    if uploaded_files and submitted and not prompt:
        st.info("Please input a prompt")
    
    if prompt and submitted and not uploaded_files:
        prompt_without_article = f'If I ask you who you are, please answer directly "I am GBB AI, a large language model powered by Google Gemini." If I ask you about "GBB", please answer directly "GBB stands for Go Beyond Blue, which means "deep blue"." If I haven\'t asked you who you are or about "GBB", then you don\'t need to answer the previous content, nor do you need to quote the previous content. Please answer me directly based on the following "prompt":\n\n Prompt: \n{prompt}\n\nAnswer:'
        with st.spinner('Please wait :coffee: Content is coming soon...'):
            generated_text = generate_text(prompt_without_article)
            st.write(generated_text)
            
    if prompt and submitted and uploaded_files:
        prompt_with_article = f'If I ask you who you are, please answer directly "I am GBB AI, a large language model powered by Google Gemini." If I ask you about "GBB", please answer directly "GBB stands for Go Beyond Blue, which means "deep blue"." If I haven\'t asked you who you are or about "GBB", then you don\'t need to answer the previous content, nor do you need to quote the previous content. Please answer me directly based on the following "prompt":\n{all_text}\n\n Prompt: \n{prompt}\n\nAnswer:'
        with st.spinner('Please wait :coffee: Content is coming soon...'):
            generated_text = generate_text(prompt_with_article)
            st.write(generated_text)
